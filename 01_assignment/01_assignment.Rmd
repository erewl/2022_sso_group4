---
title: "Assignment 1 - Group 4"
author: "Katrin Grunert, Carmen Byrne Salsas, Bono Lardinois"
date: "26 September 2022"
output: pdf_document
fontsize: 11pt
highlight: tango
---

`r weather = read.table("weather.txt",header=TRUE)`
`r birthweight = read.table("birthweight.txt",header=TRUE)`
`r austen = read.table("austen.txt",header=TRUE)`


# Exercise 1.1

## Exercise 1.1.a


The point estimate for the population mean is `r round(mean(birthweight$birthweight), 3)`.

For the Shapiro-Wilk-Test we formulate the null and alternative hypotheses as follows:

$H_0$:  The data is normally distributed.

$H_1$: The data is not normally distributed.

Since the p-value of `r round(shapiro.test(birthweight$birthweight)$p.value, 4)` is greater than 0.05, we will not reject the null hypothesis, stating the data may be normally distributed. 

In the QQPlot a straight line can be drawn through most data points, and the Histogram shows a bell-curve-like distribution of the data, suggesting, that the data is approximately normally distributed. 


```{r,echo=FALSE,fig.height=3.5}
par(mfrow=c(1,2))
qqnorm(birthweight$birthweight, main="QQPlot of Birthweight")
qqline(birthweight$birthweight, col = "red")
hist(birthweight$birthweight, main="Histogram of Birthweight")
```

## Exercise 1.1.b


```{r, echo=FALSE}
n = nrow(birthweight); mu = mean(birthweight$birthweight); s = sd(birthweight$birthweight)
```

sample size n = `r round(n, 3)`; population $\mu$ = `r round(mu, 3)`; sample standard deviation = `r round(s, 3)`; $\alpha$ = 0.05 (90% CI two-sided)

```{r}
alpha = (1-0.9)/2
n = nrow(birthweight); mu = mean(birthweight$birthweight)
s = sd(birthweight$birthweight)
error_margin = qt(p=alpha, df=n-1, lower.tail = FALSE)*s/sqrt(n)

lower_bound = mu - error_margin 
upper_bound = mu + error_margin
```

The 90% Confidence Interval is constructed with the help of the student t-distribution. It has the lower and upper bounds of [`r round(lower_bound, 3)`, `r round(upper_bound, 3)`], meaning we can say with 90% confidence that the population mean lies between these values. 

## Exercise 1.1.c
For the t-test we formulate the null and alternative hypotheses as follows:

$H_0$: mean_birthweight = 2800

$H_1$: mean_birthweight > 2800

This is also known as a right-tailed test.

```{r}
t.test(birthweight$birthweight, mu = 2800, alternative = "greater")
```
The p-value (0.0136) of the t-test is less than the significance level of 0.05, and therefore we reject the null hypothesis.
We have sufficient evidence to verify the expert's claim that the mean birthweight is greater than 2800.


## Exercise 1.1.d
The Student T-Test from Exercise 1.1.c is one-tailed because we were interested in whether the birthweight mean is bigger than 2800, resulting in a right-tailed and not a two-tailed test.


# Exercise 1.2
## Exercise 1.2.a

From a fictive sample (n = 200) of working parents, 140 parents receive childcare benefits. This report section will investigate the fraction (p) of working parents that receive childcare benefitsâ€”starting with calculating the point estimate for p. The point estimate of p is the chance of success. In this case, success means having child benefits. This is done by dividing the number of parents with child benefits (140) by the total sample size (200). In this case, this will be:

$\hat{p}$ = 140/200 = `r round(140/200,3)`

## Exercise 1.2.b

```{r, echo=FALSE}
n = 200
p_hat = 140/n
q_hat = 1 - p_hat
```

To derive a 99% confidence interval for the $\hat{p}$, we first need to calculate the chance of failure (q). To estimate the chance of failure (no child benefit), we use the formula $\hat{q} = 1 - \hat{p}$. In this case, the $\hat{q}$ is `r round(1-p_hat,3)`.


```{r, results='hide'}
alpha = 0.01
z_alpha = qnorm(1 - (alpha/2))
round(z_alpha, 3)
```

Next, we can derive the z-value, `z_alpha`, with the `q-norm` function from R. When we fill in the alpha level of 0.99, we get a z-alpha of `r round(z_alpha,3)`.

```{r, results='hide'}
upper_bound = p_hat + (z_alpha)*sqrt((p_hat*q_hat)/n)
lower_bound = p_hat - (z_alpha)*sqrt((p_hat*q_hat)/n)
```

With the z-value, the probability of success $\hat{p}$, the probability of failure $\hat{q}$, and the sample size, we can calculate the upper and lower bound of the 99% confidence interval. This results in the following confidence interval [`r round(lower_bound,3)`, `r round(upper_bound,3)`].

## Exercise 1.2.c

Finally, we will test the null hypothesis that the true fraction of working parents who receive childcare benefits is equal to 75%. Namely, $H_0: p = p_0 = 0.75$, $H_1: p \neq 0.75$.

To test this, we will make use of a binomial test. The binomial test can be used to determine whether the population proportion of one level in a dichotomous variable equals a specific claimed value. In our case, we are dealing with a dichotomous variable because parents either have child benefits or do not.

For the binomial test, we need the following variables: number of success (140), sample size (200), and the hypothesized proportion of working parents who receive childcare benefits (0.75).

The p-value for this binomial test is approximately equal to 0.103 independently of the significance level.

The first test was conducted at a significance level $\alpha$ equal to 0.1, conveying a confidence interval (CI) of (1 - 0.1) 90%. Since the p-value (0.103) is larger than the significance level (0.01) we fail to reject the null hypothesis. In addition, the test resulted in a confidence interval of [0.642, 0.753]. We can see that $p_0 = 0.75$ falls inside this interval. This supports our previous assumption that there is not enough evidence to reject the null hypothesis at this significance level. Note that $p_0$ is only barely included in the confidence interval.

The second test used a significance level $\alpha$ equal to  0.15, yielding the following 85% confidence interval: [0.649, 0.747]. In this case, the p-value (0.103) is smaller than the significance level (0.15) which suggests that there is enough evidence to reject the null hypothesis, this is supported by the fact that the confidence interval does not contain the p-value (0.75). However, this conclusion should be taken with a grain of salt since generally an 85% confidence level is not considered sufficiently accurate to provide any reliable conclusions.

The third test was conducted at a significance level equal to 0.01, yielding the following 99% confidence interval: [0.610, 0.780]. In this case, the p-value (0.103) is larger than the significance level and the confidence interval contains the hypothesized true proportion of working parents with childcare benefits, which suggests that there is not enough evidence to reject the null hypothesis $H_0$.



```{r}
test_90 = binom.test(140,200, p = 0.75, conf.level = 0.9)
test_85 = binom.test(140,200, p = 0.75, conf.level = 0.85)
test_99 = binom.test(140,200, p = 0.75, conf.level = 0.99)

tab <- matrix(c(
  round(test_99$conf.int[1], 3), 
  round(test_90$conf.int[1], 3), 
  round(test_85$conf.int[1], 3), 
  round(test_99$conf.int[2], 3), 
  round(test_90$conf.int[2], 3), 
  round(test_85$conf.int[2], 3)), ncol=2,nrow = 3, byrow=TRUE)
colnames(tab) <- c("lower", "Upper")
rownames(tab) <- c('Alpha 0.01','Alpha 0.1','Alpha 0.15')
tab <- as.table(tab)
tab

```



# Exercise 1.3
## Exercise 1.3.a
```{r}
summary(weather$humidity)
summary(weather$temperature)
```

```{r,echo=FALSE,fig.height=3.5}
par(mfrow=c(1,2))
boxplot(weather$humidity, main='Boxplot of Humidity')
boxplot(weather$temperature, main='Boxplot of Temperature')
```
The summary table and the boxplot of the temperature data show a greater range of values (see Min and Max values) than the humidity data. Notably, we can see that the humidity values are approximately symmetrically distributed around the median. On the other hand, the values smaller than the median temperature have a larger range than those that are larger than the median.
In both summaries of humidity and temperature, no severe outliers can be found, as the mean and median are close to each other and no outliers are being displayed in the boxplots.

When plotting both data in a scatterplot (below), no clear correlation can be found between humidity and temperature.

```{r,echo=FALSE,fig.height=4.5, fig.width=4.5}
par(mfrow=c(1,1))
plot(weather$humidity, weather$temperature, xlab='Humidity', ylab="Temperature", main="Scatterplot of HumidityxTemperature")
```


## Exercise 1.3.b

Histogram and QQPlot of the temperature data suggest that the data is not normally distributed, neither a clear bell curve can be drawn into the histogram, nor a straight line can be fitted to approximately all data points in the QQPlot.

```{r,echo=FALSE,fig.height=3.5}
par(mfrow=c(1,2))
hist(weather$temperature, main = 'Histogram of Temperature')
qqnorm(weather$temperature, main = 'QQPlot of Temperature')
qqline(weather$temperature, col = "red")
```


## Exercise 1.3.c

```{r}
n = nrow(weather); mu = mean(weather$temperature); s = sd(weather$temperature)
error_margin = qt((1-0.90)/2,df=n-1, lower.tail=FALSE)*s/sqrt(n)

lower_bound = mu - error_margin 
upper_bound = mu + error_margin
```

Assuming normality of the data, we can say with 90% confidence that the mean temperature of the population lays in the range of [`r round(lower_bound, 3)`, `r round(upper_bound, 3)`]

## Exercise 1.3.d
For a large sample size $n$, a t-statistic at a significance level $\alpha$ can be approximated by a z-statistic. In our case, this is a necessary step since the sample size is unknown; therefore, we cannot specify in advance the degrees of freedom of the t-statistic.  Therefore, let's assume the sample mean is normally distributed. The formula for a 95% confidence interval around the mean $\mu$  with standard deviation $\sigma$ is: 
$\mu \pm E$, where E is the margin of error defined as $E = z_{\alpha/2} \frac{\sigma}{\sqrt n}$. 

For a large sample size, the population standard deviation $\sigma$ is approximately equal to the sample standard deviation $s$. Solving for $n$, we find the following expression for the minimum sample size for a confidence interval of length $2 E$ at a significance level $\alpha$:

$n \geq \frac{(z_{\alpha/2} s)^2}{E^2}$

```{r}
ci_length = 2
E = ci_length/2
alpha = 0.05
z_stat = qnorm(alpha/2, lower.tail=FALSE)
s_humidity = sd(weather$humidity)
n = (z_stat^2)*(s_humidity^2)/E^2
```

Thus, the minimum sample size for a 95% confidence interval for the mean humidity with maximum length 2% is `r ceiling(n)`.


# Exercise 1.4
## Exercise 1.4.a

We can see four columns and six rows in the dataset described below. The respective columns mean the following: 

- Sense is chapters 1 and 3 of the book Sense and Sensibility by Jane Austen.
- Emma is chapters 1,2, and 3 from the book Emma by Jane Austen.
- Sand1 is chapters 1 and 6 of Sanditon by Jane Austen.
- Sand2 is chapters 12 and 24, Sanditon by an admirer. 
```{r}
austen
```

Testing for independence or homogeneity depends on the topic of interest. 
Since the admirer attempts to imitate Austen's writing style, the contingency table test for homogeneity seems a fitting choice. Generalized, the contingency table test for homogeneity explores whether two datasets are originating from the same population, this maps well to the intention of the admirer. In our case, we want to test for consistency between the different samples, so we want to see whether the columns are the same or different. We will use the Homogeneity test to look for a difference between the columns.


## Exercise 1.4.b


To test whether Austen was consistent in her use of words throughout the writing of her books, we performed a chi-squared test. The H0, in this case, is that row categories are the same for each column. The H1 is that the distribution among the row categories is not the same for each column.

The chi-squared test is used to compare observed results and expected results. Its purpose is to discover whether there is a significant difference between the observed and the expected data.

To check the condition of the chi-squared test: At least 80% of the expected values should be at least 5. We took the expected table from the chi-squared test and made a boolean statement (Expected > 5). This table showed that every cell has a value greater than 5, meaning the condition is met.  

```{r, echo=FALSE}
df_1 <- austen[c("Sense","Emma", "Sand1")]
austen_chi_1 = chisq.test(df_1)
```

```{r}
austen_chi_1$expected > 5
```

After the precondition for the chi-squared test is verified, we create a subset with only the books/chapters that were written by Austen, to test whether she was consistent in her own writing. With this new dataset, we perform a chi-squared test. This gives us a p-value of 0.161. This p-value is > 0.05, meaning we cannot reject the null hypothesis and state that there is a significant difference between the different columns (books/chapters) in the data frame. 

```{r}
df_1 <- austen[c("Sense","Emma", "Sand1")]
austen_chi_1 = chisq.test(df_1)
round(austen_chi_1$p.value, 3)
```

Next, we will determine where the main inconsistencies lay. Inconsistencies can be defined as where the expected values diverge the most from the observed values, these differences are also called residuals.

```{r}
residuals(austen_chi_1)
```

The inconsistencies lay in Sense in the words 'without' and 'a', in Emma the main inconsistency was the word 'without', and lastly, in Sanditon the two main inconsistencies were the words 'that' and 'a'. The sign of a residual indicates whether a word has been used more often or less often compared to the other books.

## Exercise 1.4.c

To check whether the admirer was successfully imitating Austen's writing style, we can compare the chapters from the Sanditon book. For this, we made a new data frame and performed another chi-squared test. Here the p-value was 0.131. This p-value is greater than 0.05, meaning we can not say there is a significant difference between the chapters written by Austen and her admirer based on the given data. Moreover, the condition of the chi-squared was met because all cells of the expected table were greater than 5. 
```{r}
df_2 <- austen[c("Sand1", "Sand2")]
austen_chi_2 = chisq.test(df_2)
round(austen_chi_2$p.value, 3)
```
```{r}
austen_chi_2$expected > 5
```

As in 1.4.b, we will use the residuals to determine the main inconsistencies between the Austen's admirer and the original author.

```{r}
residuals(austen_chi_2)
```

The residuals table above shows, that the admirer was using the word 'without' less than Austen and the word 'an' more often than the original author.